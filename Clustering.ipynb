{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCF5YSl62QAeBA6ZEXq6Oc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qu1nnD/CS290/blob/main/Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g_X-U2ooQUtd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ZEQ19coxN1Yh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allow for two centroid initialization options: either specify the initial centroids or select them randomly from the dataset being used. Experiment with both options to see how the final centroids depend on the initialization.\n",
        "\n",
        "When applying your algorithm, plot the data and the current centroids at each iteration. This provides a nice visualization of the algorithm in action.\n",
        "\n",
        "With both the iris and penguins datasets, you actually have labels and therefore know the correct number of clusters, ùëò. Experiment with different values of ùëò to see what happens, since the ‚Äúcorrect‚Äù value isn‚Äôt actually known in practice."
      ],
      "metadata": {
        "id": "Ml0IOotbQLwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "penguins = pd.read_csv(\"https://github.com/mbrudd/csci290/raw/refs/heads/main/data/penguins.csv\")\n",
        "penguins.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "vuEb4NNHWogP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "penguins.info()"
      ],
      "metadata": {
        "id": "x4LI41gsWodW",
        "outputId": "2de616bc-7d52-4a9f-8088-fc0ca2c5fabd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 333 entries, 0 to 343\n",
            "Data columns (total 8 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   species            333 non-null    object \n",
            " 1   island             333 non-null    object \n",
            " 2   bill_length_mm     333 non-null    float64\n",
            " 3   bill_depth_mm      333 non-null    float64\n",
            " 4   flipper_length_mm  333 non-null    float64\n",
            " 5   body_mass_g        333 non-null    float64\n",
            " 6   sex                333 non-null    object \n",
            " 7   year               333 non-null    int64  \n",
            "dtypes: float64(4), int64(1), object(3)\n",
            "memory usage: 23.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ini_Centroids(k, dataset, centroid_type, features):\n",
        "  if(centroid_type == \"random\"): # Makes the centroid random\n",
        "    indices = np.random.choice(len(features), size=k, replace=False)\n",
        "    return features.to_numpy()[indices]\n",
        "  elif(centroid_type == 'arthur'): #arthur vestile method\n",
        "    centroids = []\n",
        "    first_centroid = features.sample(n=1)\n",
        "    centroids.append(first_centroid.values[0])\n",
        "    for x in range(1, k):\n",
        "      distances = np.array([min(np.linalg.norm(x - np.array(centroid), axis=0) ** 2 for centroid in centroids) for x in features.to_numpy()])\n",
        "      total_distance = distances.sum()\n",
        "      probabilities = distances / total_distance  # Probability: D(x_i)^2 / sum(D(x_j)^2)\n",
        "      chosen_idx = np.random.choice(len(features), p=probabilities)\n",
        "      centroids.append(features.iloc[chosen_idx].values)\n",
        "    return np.array(centroids)\n",
        "  else: # Makes the centroid farthest away from everything\n",
        "    centroids = [features.sample(n=1).values[0]]  # Choose the first centroid randomly\n",
        "    for x in range(1, k):\n",
        "        distances = np.array([min(np.linalg.norm(point - np.array(c)) for c in centroids)for point in features.to_numpy()])\n",
        "        farthest_idx = np.argmax(distances)\n",
        "        centroids.append(features.iloc[farthest_idx].values)\n",
        "    return np.array(centroids)"
      ],
      "metadata": {
        "id": "coYBqOuxBTLm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inertia(features, labels, centroids): #sum of the squared distances from each instance to its centroid\n",
        "    total_distance = 0\n",
        "    for i in range(len(centroids)):\n",
        "        cluster_points = features[labels == i]\n",
        "        distances = np.linalg.norm(cluster_points - centroids[i], axis=1)  # Distances to centroid\n",
        "        total_distance += np.sum(distances ** 2)  # Sum of squared distances\n",
        "    return total_distance"
      ],
      "metadata": {
        "id": "GdShzkCfBMzV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sil_coe(dataset, centroid_type, feature_x, feature_y, target):\n",
        "    plt.figure(figsize=(11,9))\n",
        "  for k in (3,4,5,6):\n",
        "    plt.subplot(2,2,k-2)\n",
        "    y_pred=kmeans_per_k[k-1].labels\n",
        "    coeffs=silhouette_coefficients[y_pred==1]\n",
        "    coeffs.sort()"
      ],
      "metadata": {
        "id": "zObPMD2tBYBA"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(true_labels, predicted_labels):\n",
        "    correct_predictions = np.sum(true_labels == predicted_labels)\n",
        "    return correct_predictions / len(true_labels)"
      ],
      "metadata": {
        "id": "Ly-Brf10CmbA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def K_Means(k, dataset, centroid_type, feature_x, feature_y, target):\n",
        "    converged = False\n",
        "    features = dataset[[feature_x, feature_y]].dropna()\n",
        "    centroids=ini_Centroids(k, dataset, centroid_type, features) # initializes the first centroids\n",
        "    while not converged: # Loops until there isn't massive changes between centroids and new_centroids\n",
        "        #Calculations\n",
        "        distances = np.linalg.norm(features.to_numpy()[:, np.newaxis] - centroids, axis=2) # calcuates the eudclidean distance between data points and centroids\n",
        "        labels = np.argmin(distances, axis=1) # assigns data points to the closest centroid\n",
        "        new_centroids = np.array([features[labels == i].mean(axis=0) if np.any(labels == i) else centroids[i] for i in range(k)]) # calculates updated centroids given the mean of all data points from each cluster\n",
        "        #Ploting\n",
        "        '''\n",
        "        plt.figure()\n",
        "        plt.scatter(features[feature_x], features[feature_y], c=labels, marker='o') # Creates the circles that represent data points on the scatter plot\n",
        "        for c in centroids:\n",
        "          plt.scatter(c[0], c[1], c='red',marker='X', s=200) # Creates the red X markers that show where the centroids are on the scatter plot\n",
        "        plt.title(f'Current Centroids for k={k}') # Creates the title of the scatter plot\n",
        "        plt.xlabel(feature_x) # Creates the x axis label for the scatter plot\n",
        "        plt.ylabel(feature_y) # Creates the y axis label for the scatter plot\n",
        "        plt.show() # Prints the scatter plot\n",
        "        '''\n",
        "        #Convergence test and updating centroids\n",
        "        converged = np.all(np.abs(centroids - new_centroids) <= 0.001) # Tests to see if there is major differences between the current and new centroids to see if it needs to continue the loop again\n",
        "        centroids = new_centroids\n",
        "    label_mapping = {}\n",
        "    for i in range(k):\n",
        "        cluster_indices = np.where(labels == i)[0]\n",
        "        if len(cluster_indices) > 0:\n",
        "            majority_label = dataset.iloc[cluster_indices][target].mode().iloc[0]\n",
        "            label_mapping[i] = majority_label\n",
        "    # Replace numerical labels with string labels\n",
        "    string_labels = [label_mapping[label] for label in labels]\n",
        "    #Add making the predicted labels have a string value\n",
        "    return string_labels, centroids"
      ],
      "metadata": {
        "id": "EEYzemPTWoX6"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(3,4):\n",
        "  print(f\"Running K-Means with k={k}\")\n",
        "  labels, clusters = K_Means(k, penguins, \"random\",'flipper_length_mm','bill_length_mm','species')\n",
        "  true_labels = penguins['species']\n",
        "  species_names = penguins['species'].unique()\n",
        "  a = accuracy(true_labels, labels)\n",
        "  print(f'Accuracy: {a:.2f}')"
      ],
      "metadata": {
        "id": "bO11T8YmWoSL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ab34e9e-3706-45e6-ecc7-dca131fe3b2d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Means with k=3\n",
            "Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(3,4):\n",
        "  print(f\"Running K-Means with k={k}\")\n",
        "  labelsA, clusters = K_Means(k, penguins, \"arthur\", 'flipper_length_mm', 'bill_length_mm','species')\n",
        "acc = accuracy(true_labels, labelsA)\n",
        "print(f'Accuracy: {acc:.2f}')"
      ],
      "metadata": {
        "id": "E2VJRUuOWoPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "438a49ce-2376-4860-f0af-ab5600b239f1"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Means with k=3\n",
            "Accuracy: 0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(3,4):\n",
        "  print(f\"Running K-Means with k={k}\")\n",
        "  labelsF, clusters = K_Means(k, penguins, \"far\", 'flipper_length_mm', 'bill_length_mm','species')\n",
        "accurate = accuracy(true_labels, labelsF)\n",
        "print(f'Accuracy: {accurate:.2f}')"
      ],
      "metadata": {
        "id": "I-Ir46StWoMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51182196-c3c2-4b84-e4ff-f2286f77335c"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running K-Means with k=3\n",
            "Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sil_coe(penguins[['flipper_length_mm', 'bill_length_mm']], 'random', 'flipper_length_mm', 'bill_length_mm','species')"
      ],
      "metadata": {
        "id": "wzhr3CFP5NWO"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the examples in Chapter 9 of Hands-on Machine Learning to fit Gaussian Mixture Models to both the iris and penguins datasets. After fitting GMMs to these datasets,\n",
        "\n",
        "Plot the centers of the Gaussian distributions in your GMM along with the centroids from your ùëò-means modeling results. How different are they?\n",
        "\n",
        "Calculate the overall accuracy of each GMM, and compare with the overall accuracy of your ùëò-means models."
      ],
      "metadata": {
        "id": "iutDvQRp9sm-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZoIowhT9mRk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}